# 因果推論と操作変数法 - 練習問題

**対応レクチャー**: lecture_20251106.pdf
**難易度**: ★★★★(最上級)
**目標時間**: 120分
**重要度**: S級(試験超重要・中間試験範囲)

---

## Part 1: 潜在的結果モデルと因果効果

### 問題1: 潜在的結果の基本概念

ある職業訓練プログラムの効果を調べる研究を考える。個人 i について：
- D_i = 1: 訓練を受けた
- D_i = 0: 訓練を受けなかった
- y_{1i}: 訓練を受けた場合の賃金（潜在的結果）
- y_{0i}: 訓練を受けなかった場合の賃金（潜在的結果）

(1) 観測される賃金 y_i を、D_i と潜在的結果 y_{1i}, y_{0i} を用いて表しなさい。

(2) 個人 i の処置効果 τ_i を定義し、これを直接観測できない理由（因果推論の根本的問題）を説明しなさい。

(3) 以下のデータが得られたとする：

| 個人 | D | 観測された賃金 y |
|------|---|----------------|
| 1    | 1 | 500万          |
| 2    | 0 | 300万          |
| 3    | 1 | 480万          |
| 4    | 0 | 320万          |

単純に E[y | D=1] - E[y | D=0] を計算すると訓練の効果は何万円と推定されるか。また、この推定値が真の因果効果と一致しない可能性がある理由を説明しなさい。

#### 📖 ヒント
- 各個人について、y_{1i} と y_{0i} のうち片方しか観測できない
- y_i = (1-D_i)y_{0i} + D_i y_{1i}
- 選択バイアスの可能性を考える

#### ✅ 解答

**(1) 観測される賃金の表現**

```
y_i = (1-D_i)y_{0i} + D_i y_{1i}
```

または

```
y_i = { y_{1i}  (D_i = 1 のとき)
      { y_{0i}  (D_i = 0 のとき)
```

**解釈**:
- D_i = 1（訓練を受けた）の場合、y_i = y_{1i} が観測される
- D_i = 0（訓練を受けなかった）の場合、y_i = y_{0i} が観測される
- 各個人について、2つの潜在的結果のうち1つだけが実現し、観測される

**(2) 個人の処置効果と観測できない理由**

**個人の処置効果**:
```
τ_i = y_{1i} - y_{0i}
```

**因果推論の根本的問題（Fundamental Problem of Causal Inference）**:

同じ個人 i について、y_{1i} と y_{0i} の**両方を同時に観測することは不可能**。

**理由**:
1. 同じ個人が、同じ時点で、2つの異なる状態（訓練を受けた・受けなかった）に同時に存在することはできない
2. もし D_i = 1 なら、y_{1i} が観測され、y_{0i} は**反実仮想（counterfactual）**となる
3. もし D_i = 0 なら、y_{0i} が観測され、y_{1i} は**反実仮想**となる
4. タイムマシンがない限り、観測されない方の結果を知ることはできない

**結果**: 個人レベルの因果効果 τ_i を直接計算することは不可能

**(3) 単純比較の計算と問題点**

**計算**:

訓練を受けた人（D=1）:
```
E[y | D=1] = (500 + 480) / 2 = 490万
```

訓練を受けなかった人（D=0）:
```
E[y | D=0] = (300 + 320) / 2 = 310万
```

**単純比較**:
```
E[y | D=1] - E[y | D=0] = 490 - 310 = 180万
```

**問題点：この180万は真の因果効果ではない可能性**

**理由（選択バイアス）**:
1. **自己選択**: やる気のある人、能力の高い人が訓練を受ける傾向
2. **訓練を受けた人は、訓練がなくても賃金が高かった可能性**
   - E[y_0 | D=1] > E[y_0 | D=0]（受けた人は、受けなくても元々賃金が高い）
3. **真の効果と選択バイアスが混在**:
```
E[y | D=1] - E[y | D=0]
= E[y_1 | D=1] - E[y_0 | D=0]
= {E[y_1 | D=1] - E[y_0 | D=1]} + {E[y_0 | D=1] - E[y_0 | D=0]}
  ↑                              ↑
  ATT（真の効果）                選択バイアス
```

4. **180万の内訳**:
   - 訓練の真の効果: 不明
   - 元々の能力差: 不明
   - これらが混ざっている

**結論**: 単純比較では因果効果を正しく推定できない

---

### 問題2: ATE, ATT, ATU の違い

以下の3つの因果効果の指標について説明しなさい。

(1) **ATE（Average Treatment Effect; 平均処置効果）**を定義し、その政策的意味を説明しなさい。

(2) **ATT（Average Treatment Effect on the Treated; 処置群の平均処置効果）**を定義し、ATEとの違いを説明しなさい。

(3) ATE ≠ ATT となる具体例を挙げなさい。

#### 📖 ヒント
- ATE = E[y_1 - y_0]
- ATT = E[y_1 - y_0 | D=1]
- ATU = E[y_1 - y_0 | D=0]
- 処置を受ける人と受けない人の特性の違いを考える

#### ✅ 解答

**(1) ATE（平均処置効果）**

**定義**:
```
ATE = E[y_1 - y_0] = E[y_1] - E[y_0]
```

**意味**:
- **集団全体**（母集団）における処置の平均的な効果
- 「もし全員が訓練を受けたら」の平均賃金と、「もし全員が訓練を受けなかったら」の平均賃金の差

**政策的意味**:
- 「この介入を全員に実施すべきか？」という政策判断の基準
- 社会全体での期待される平均的な効果
- ATE > 0 なら、平均的には介入に効果がある
- 費用対効果分析の基礎となる指標

**例**:
- 国民全員に職業訓練を提供した場合の期待される平均的な賃金上昇効果
- この訓練プログラムを全国展開すべきかの判断材料

**(2) ATT（処置群の平均処置効果）**

**定義**:
```
ATT = E[y_1 - y_0 | D=1] = E[y_1 | D=1] - E[y_0 | D=1]
```

**意味**:
- **実際に処置を受けた人たち**における平均的な効果
- 「訓練を受けた人たちが、もし受けなかったら」との比較

**ATEとの違い**:

| 項目 | ATE | ATT |
|------|-----|-----|
| 対象集団 | 母集団全体 | 処置を受けた人のみ |
| 意味 | 全員に介入した場合の効果 | 実際の参加者への効果 |
| 政策的意味 | 「全員に実施すべきか」 | 「現在の参加者にとって有益か」 |
| 関係 | 一般的に ATE ≠ ATT | RCTなら ATE = ATT |

**なぜ異なるか**:
- 処置を受ける人は、ランダムに選ばれているわけではない
- 処置を受ける人と受けない人は、異なる特性を持つ可能性がある
- 処置効果が個人によって異なる（異質な処置効果）場合、ATTとATEは一致しない

**(3) ATE ≠ ATT となる具体例**

**例: 職業訓練プログラム**

**状況**:
- やる気のある人、向上心の強い人が訓練を自己選択して受ける
- やる気のない人、訓練に興味がない人は受けない

**個人別の効果（仮想的な真の値）**:

| 個人 | タイプ | y_0 | y_1 | τ = y_1 - y_0 | 実際の選択 D |
|------|-------|-----|-----|---------------|------------|
| A    | やる気あり | 400 | 550 | 150 | 1 |
| B    | やる気あり | 450 | 600 | 150 | 1 |
| C    | やる気なし | 300 | 350 | 50  | 0 |
| D    | やる気なし | 280 | 320 | 40  | 0 |

**ATE（全員の平均）**:
```
ATE = (150 + 150 + 50 + 40) / 4 = 97.5万
```

**ATT（訓練を受けた人の平均）**:
```
ATT = (150 + 150) / 2 = 150万
```

**ATU（訓練を受けなかった人の平均）**:
```
ATU = (50 + 40) / 2 = 45万
```

**結果**: ATE (97.5) ≠ ATT (150) ≠ ATU (45)

**解釈**:
- やる気のある人（A, B）は訓練から大きな効果を得る（150万）
- やる気のない人（C, D）は訓練からの効果が小さい（40-50万）
- **自己選択の結果、効果が大きい人が訓練を受ける**
- ATT > ATE > ATU という関係になる

**政策的含意**:
- 現在の参加者には効果が大きい（ATT = 150万）
- しかし、全員に強制したら効果は小さくなる（ATE = 97.5万）
- 非参加者に強制しても効果はさらに小さい（ATU = 45万）

---

## Part 2: RCTと観察データでの因果推論

### 問題3: RCT（ランダム化比較試験）の理論

RCTでは、処置の割り当て D を研究者がランダムに決定する。

(1) RCTにおける基本的な仮定を数式で表し、その意味を説明しなさい。

(2) この仮定のもとで、ATE = E[y | D=1] - E[y | D=0] と単純化できることを証明しなさい。

(3) 回帰モデル y_i = β_0 + τD_i + u_i をOLSで推定した場合、τ̂ が ATE の一致推定量となる理由を説明しなさい。

#### 📖 ヒント
- RCTの仮定: D ⊥ (y_0, y_1)
- 独立性から、E[y_1 | D=1] = E[y_1] などが導ける
- OLSの仮定 E[u | D] = 0 が成立する

#### ✅ 解答

**(1) RCTの基本的な仮定**

**仮定**:
```
D ⊥ (y_0, y_1)
```

または

```
(y_0, y_1) ⊥ D
```

**記号の意味**:
- ⊥ は「独立」を表す
- D: 処置の割り当て（0 or 1）
- y_0, y_1: 潜在的結果

**意味**:
1. **処置の割り当て D と潜在的結果 (y_0, y_1) が独立**
2. D はサイコロを振るように完全にランダムに決まる
3. 個人の特性（能力、やる気、年齢など）とは無関係に処置が割り当てられる
4. 訓練を受けるグループと受けないグループは、**平均的には同じ特性**を持つ

**具体例**:
- コイントス、乱数表、くじ引きなどで処置を割り当て
- 「この人は効果が出そう」「この人は若いから」などの判断を一切入れない
- 結果として、処置群と対照群は比較可能になる

**(2) ATEの単純化の証明**

**Step 1**: ATEの定義
```
ATE = E[y_1] - E[y_0]
```

**Step 2**: 独立性の利用

RCTの仮定 D ⊥ (y_0, y_1) より:
```
E[y_1 | D=1] = E[y_1 | D=0] = E[y_1]
E[y_0 | D=1] = E[y_0 | D=0] = E[y_0]
```

**解釈**: D の値に関わらず、潜在的結果の期待値は同じ

**Step 3**: 観測可能な量への変換

観測される y は:
```
y = (1-D)y_0 + D y_1
```

D=1 のとき:
```
E[y | D=1] = E[y_1 | D=1]
```

D=0 のとき:
```
E[y | D=0] = E[y_0 | D=0]
```

**Step 4**: 独立性を使った置き換え

Step 2 の結果を使うと:
```
E[y_1] = E[y_1 | D=1] = E[y | D=1]
E[y_0] = E[y_0 | D=0] = E[y | D=0]
```

**Step 5**: ATEの単純化
```
ATE = E[y_1] - E[y_0]
    = E[y | D=1] - E[y | D=0]
```

**結論**: RCTのもとでは、ATEは単純に「処置群の平均賃金」から「対照群の平均賃金」を引くだけで推定できる

**(3) OLS推定量が一致推定量となる理由**

**回帰モデル**:
```
y_i = β_0 + τD_i + u_i
```

**Step 1**: 誤差項の定義

潜在的結果モデルから:
```
y_i = (1-D_i)y_{0i} + D_i y_{1i}
    = y_{0i} + D_i(y_{1i} - y_{0i})
```

これを回帰式と比較すると:
```
β_0 = E[y_0]
τ = E[y_1 - y_0] = ATE
u_i = y_{0i} - E[y_0]
```

**Step 2**: E[u | D] = 0 の確認

RCTの仮定 D ⊥ (y_0, y_1) より:
```
E[u_i | D_i] = E[y_{0i} - E[y_0] | D_i]
             = E[y_{0i} | D_i] - E[y_0]
             = E[y_{0i}] - E[y_0]  （独立性より）
             = 0
```

**Step 3**: OLSの性質

E[u | D] = 0 が成立するので:
- OLSの最も重要な仮定が満たされる
- τ̂ (OLS推定量) は τ (真のATE) の**不偏推定量**
- n → ∞ で τ̂ → τ（**一致推定量**）

**結論**:
- RCTでは、単純なダミー変数回帰のOLS推定量が因果効果を正しく推定する
- 複雑な統計手法は不要
- これがRCTが「因果推論のゴールドスタンダード」と呼ばれる理由

---

### 問題4: 条件付き独立性の仮定（CIA）

観察データでは RCT の仮定 D ⊥ (y_0, y_1) は成立しないが、交絡変数 X を条件付ければ独立になると仮定する。

(1) **CIA（Conditional Independence Assumption; 条件付き独立性の仮定）**を数式で表し、その意味を詳しく説明しなさい。

(2) **オーバーラップの仮定**を数式で表し、なぜこの仮定が必要なのか説明しなさい。

(3) この2つの仮定のもとで、条件付きATE τ(X) = E[y_1 | X] - E[y_0 | X] が E[y | D=1, X] - E[y | D=0, X] として推定できることを示しなさい。

#### 📖 ヒント
- CIA: (y_0, y_1) ⊥ D | X
- オーバーラップ: 0 < P(D=1|X) < 1
- CIAを使って条件付き期待値を変形する

#### ✅ 解答

**(1) CIA（条件付き独立性の仮定）**

**数式**:
```
(y_0, y_1) ⊥ D | X
```

または

```
D ⊥ (y_0, y_1) | X
```

**意味**:

**言葉での表現**: 「X を条件付ければ（X の値を固定すれば）、処置の割り当て D と潜在的結果 (y_0, y_1) は独立になる」

**詳しい説明**:

1. **全体では独立ではない**:
   - 観察データでは、D と (y_0, y_1) は相関している
   - やる気のある人が訓練を受ける（自己選択）
   - D と (y_0, y_1) ⊥ は成立しない

2. **X を条件付ければ独立**:
   - X = {年齢, 学歴, 訓練前収入, やる気スコア, ...}
   - 「年齢30歳、大卒、訓練前収入300万、やる気スコア80」という**全く同じ X** を持つ人たちの中では
   - 訓練を受けた人と受けなかった人は（観測されない能力なども含めて）**平均的には同じ**
   - その X のグループ内では、D の割り当てはランダムと見なせる

3. **数式での表現**:
```
E[y_1 | D=1, X] = E[y_1 | D=0, X] = E[y_1 | X]
E[y_0 | D=1, X] = E[y_0 | D=0, X] = E[y_0 | X]
```

**CIAが意味すること**:
- 重要な交絡変数を**全て X に含めれば**、残りの選択バイアスはない
- X を条件付けることで、「疑似的なRCT」を作り出す
- X が同じ人たちの中では、処置はランダムと見なせる

**CIAの強さ**:
- これは**検証不可能な仮定**（観測できない変数の影響を無視している）
- 「本当に重要な変数を全部測定できているか？」は誰にも分からない
- 信じるしかない強い仮定

**例**:
- X に「能力」を含めていない場合、能力が高い人が訓練を受ける傾向があれば、CIAは破れる
- X に十分な変数を含めれば、CIAが成立すると**仮定**する

**(2) オーバーラップの仮定**

**数式**:
```
0 < P(D=1 | X=x) < 1   for all x
```

**意味**:

1. **どの X の値においても**:
   - 処置を受けた人（D=1）が存在する
   - 処置を受けなかった人（D=0）も存在する
   - **両方のグループが存在する**

2. **確率の範囲**:
   - P(D=1|X) = 0 ではない（全員がD=0ではない）
   - P(D=1|X) = 1 ではない（全員がD=1ではない）
   - 0 と 1 の間の値を取る

**なぜ必要か**:

**オーバーラップが破れる例**:
```
X = 年齢90歳 のとき、P(D=1 | 年齢=90) = 0
→ 90歳の人は全員が訓練を受けていない
```

この場合：
- E[y | D=1, 年齢=90] を計算したいが、**データが存在しない**
- 年齢90歳で訓練を受けた人がいないので、比較対象がない
- τ(年齢=90) を推定することは**不可能**

**別の例**:
```
X = 超高学歴 のとき、P(D=1 | 超高学歴) = 1
→ 超高学歴の人は全員が訓練を受けている
```

この場合：
- E[y | D=0, 超高学歴] を計算したいが、**データが存在しない**
- 超高学歴で訓練を受けなかった人がいないので、比較対照がない
- τ(超高学歴) を推定することは**不可能**

**結論**:
- オーバーラップがない領域では、因果効果を推定できない
- 推定は、オーバーラップが成立する X の範囲に限定される
- データで実際に確認可能な仮定

**(3) 条件付きATEの推定**

**目標**: τ(X) = E[y_1 | X] - E[y_0 | X] を観測データから推定する

**Step 1**: CIAの利用

CIA: (y_0, y_1) ⊥ D | X より:
```
E[y_1 | D=1, X] = E[y_1 | D=0, X] = E[y_1 | X]
E[y_0 | D=1, X] = E[y_0 | D=0, X] = E[y_0 | X]
```

**Step 2**: 観測可能な量への変換

D=1 のグループでは:
```
E[y | D=1, X] = E[y_1 | D=1, X]
```

CIAより:
```
E[y | D=1, X] = E[y_1 | X]
```

D=0 のグループでは:
```
E[y | D=0, X] = E[y_0 | D=0, X]
```

CIAより:
```
E[y | D=0, X] = E[y_0 | X]
```

**Step 3**: 条件付きATEの推定
```
τ(X) = E[y_1 | X] - E[y_0 | X]
     = E[y | D=1, X] - E[y | D=0, X]
```

**解釈**:
- E[y | D=1, X] は、処置を受けた人（D=1）の中で、X=x を持つ人たちの y の平均値（**観測可能**）
- E[y | D=0, X] は、処置を受けなかった人（D=0）の中で、X=x を持つ人たちの y の平均値（**観測可能**）
- この差が、条件付きATE τ(X) になる

**全体のATEの推定**:
```
ATE = E_X[τ(X)] = E_X[E[y | D=1, X] - E[y | D=0, X]]
```

X について平均（期待値）をとれば、全体の ATE が得られる。

**結論**: CIAとオーバーラップのもとで、観察データから因果効果を推定できる

---

## Part 3: 観察データでの推定方法

### 問題5: 回帰による調整（Regression Adjustment）

CIAとオーバーラップの仮定のもとで、回帰による調整で ATE を推定する方法を考える。

(1) M_1(X) = E[y | D=1, X] と M_0(X) = E[y | D=0, X] を推定する手順を説明しなさい。

(2) 推定した M̂_1(X) と M̂_0(X) から、ATE をどのように計算するか説明しなさい。

(3) この方法の利点と限界（モデルの誤定式化のリスク）を述べなさい。

#### 📖 ヒント
- D=1 のサンプルだけを使って y を X に回帰
- D=0 のサンプルだけを使って y を X に回帰
- 各個人について予測値の差を計算し、平均する

#### ✅ 解答

**(1) M_1(X) と M_0(X) の推定手順**

**M_1(X) = E[y | D=1, X] の推定**:

**Step 1**: D=1（処置を受けた人）のサンプル**だけ**を抽出

**Step 2**: y を X に回帰する（OLS、または非線形回帰）

例（線形モデルを仮定）:
```
y_i = α_0 + α_1 X_{i1} + α_2 X_{i2} + ... + ε_i
```

**Step 3**: 推定された回帰式が M̂_1(X)
```
M̂_1(X) = α̂_0 + α̂_1 X_1 + α̂_2 X_2 + ...
```

---

**M_0(X) = E[y | D=0, X] の推定**:

**Step 1**: D=0（処置を受けなかった人）のサンプル**だけ**を抽出

**Step 2**: y を X に回帰する

例（線形モデルを仮定）:
```
y_i = γ_0 + γ_1 X_{i1} + γ_2 X_{i2} + ... + η_i
```

**Step 3**: 推定された回帰式が M̂_0(X)
```
M̂_0(X) = γ̂_0 + γ̂_1 X_1 + γ̂_2 X_2 + ...
```

**注意点**:
- D=1 のグループと D=0 のグループで**別々に**回帰を実行
- 係数（α と γ）は異なる値になる可能性がある
- これは処置効果が X によって異なること（異質な処置効果）を許容している

**(2) ATEの計算方法**

**Step 1**: 各個人 i について条件付き処置効果を計算

全サンプル（D=1 と D=0 の両方）の各個人 i について:
```
τ̂(X_i) = M̂_1(X_i) - M̂_0(X_i)
```

**具体例**:
```
個人 i: 年齢=30, 学歴=16年

M̂_1(X_i) = α̂_0 + α̂_1×30 + α̂_2×16 = 500万（訓練を受けた場合の予測賃金）
M̂_0(X_i) = γ̂_0 + γ̂_1×30 + γ̂_2×16 = 450万（訓練を受けなかった場合の予測賃金）

τ̂(X_i) = 500 - 450 = 50万（この人にとっての訓練の効果）
```

**Step 2**: 全サンプルで平均してATEを計算
```
ÂTE = (1/n) Σ_{i=1}^n τ̂(X_i) = (1/n) Σ_{i=1}^n [M̂_1(X_i) - M̂_0(X_i)]
```

**手順のまとめ**:
1. 全サンプル（n人）について、その人の X を使って M̂_1(X_i) を計算
2. 全サンプル（n人）について、その人の X を使って M̂_0(X_i) を計算
3. 各人について差 τ̂(X_i) を計算
4. これを n人分全て平均する

**結果**: これが ATE の推定量 ÂTE

**(3) 利点と限界**

**利点**:

1. **異質な処置効果を捉えられる**:
   - 全員に同じ効果を仮定しない
   - 個人・グループごとに効果が異なることを許容
   - τ(X) を推定できるので、政策のターゲティングに有用

2. **柔軟性**:
   - 線形回帰だけでなく、多項式、機械学習などの非線形モデルも使える
   - X が多次元でも対応可能
   - 相互作用項なども自由に入れられる

3. **直感的**:
   - 「受けた場合」と「受けなかった場合」を別々に予測して比較
   - 解釈しやすい

4. **オーバーラップ不要**:
   - この方法はオーバーラップの仮定を必要としない
   - 予測値を使うので、データがない領域でも外挿できる（ただしリスクあり）

**限界**:

1. **CIAへの依存**:
   - CIAが成立しない（観測されない交絡変数がある）場合、推定はバイアスを持つ
   - この仮定は検証不可能
   - 「重要な変数を全部測定できているか」は誰にも分からない

2. **モデルの誤定式化のリスク**:
   - M_1(X) や M_0(X) を**正しく**推定できないと、因果効果の推定も誤る
   - 例: 真の関係が非線形なのに線形モデルを使う
   - 例: 重要な相互作用項を見落とす
   - **特定化エラー（Specification Error）**

**具体例**:
```
真のモデル: E[y | D=1, X] = 100 + 2X + 0.5X²
推定モデル: M̂_1(X) = α̂_0 + α̂_1 X  （X² を忘れた）

→ M̂_1(X) が真の E[y | D=1, X] と一致しない
→ ÂTE も間違った値になる
```

3. **高次元の X の問題**:
   - X が多数ある場合、モデルの推定が不安定
   - 多重共線性、過学習（overfitting）のリスク
   - サンプルサイズが十分でないと推定が困難

4. **外挿のリスク**:
   - オーバーラップがない領域でも予測値を使うため
   - データがない X の範囲での予測は不確実性が高い
   - 外挿（extrapolation）は危険

5. **観察データの根本的限界**:
   - RCTと比べて、因果関係の証拠としては弱い
   - 結果の解釈には慎重さが必要
   - ロバストネスチェック（様々なモデルで試す）が重要

**まとめ**:
- 回帰による調整は直感的で柔軟だが、モデルの正しい特定化に依存する
- M_1, M_0 のモデルが間違っていれば、ATE の推定も間違う
- この弱点を補うのが、次の傾向スコア法や二重ロバスト推定法

---

### 問題6: 傾向スコア・マッチング（IPW法）

傾向スコア（Propensity Score）を用いた逆確率重み付け（IPW: Inverse Probability Weighting）について説明する。

(1) 傾向スコア P(X) = P(D=1 | X) を定義し、その意味を説明しなさい。

(2) IPW推定量の導出を示し、なぜこの方法で ATE を推定できるのか説明しなさい。

(3) 傾向スコアをどのように推定するか、具体的な手順を説明しなさい。

(4) 回帰による調整と比較した場合の、IPW法の利点と限界を述べなさい。

#### 📖 ヒント
- P(X) は個人の特性 X に基づいて処置を受ける確率
- E[y_1] = E[D·y / P(X)]
- プロビットモデルやロジットモデルで推定

#### ✅ 解答

**(1) 傾向スコアの定義と意味**

**定義**:
```
P(X) = P(D=1 | X=x)
```

または

```
e(X) = P(D=1 | X)
```

**意味**:

1. **処置を受ける確率**:
   - 個人の特性 X に基づいて、その人が処置を受ける確率
   - X = {年齢, 学歴, 収入, ...} が与えられたときの、D=1 となる確率

2. **X の要約**:
   - X が多次元（例: 10個の変数）でも、P(X) は1つの数値（0と1の間）
   - 高次元の X を1次元のスコアに要約する役割

3. **具体例**:
```
個人A: 年齢=25, 大卒, 収入=250万 → P(X_A) = 0.3（訓練を受ける確率30%）
個人B: 年齢=30, 大卒, 収入=400万 → P(X_B) = 0.7（訓練を受ける確率70%）
```

4. **解釈**:
   - P(X) が高い人: 処置を受けやすい特性を持つ
   - P(X) が低い人: 処置を受けにくい特性を持つ
   - P(X) が同じ人同士は、同じような「処置を受けやすさ」を持つ

**重要な性質（Rosenbaum & Rubin, 1983）**:
```
(y_0, y_1) ⊥ D | X  ならば、
(y_0, y_1) ⊥ D | P(X)
```

つまり、CIAが X について成立すれば、P(X) について条件付けても成立する。

**(2) IPW推定量の導出**

**目標**: E[y_1] と E[y_0] を推定する

**E[y_1] の導出**:

**Step 1**: 全期待値の法則（Law of Iterated Expectations）
```
E[y_1] = E_X[E[y_1 | X]]
```

**Step 2**: CIAを使う

CIA: (y_0, y_1) ⊥ D | X より:
```
E[y_1 | X] = E[y_1 | D=1, X]
```

**Step 3**: D=1 のグループだけの期待値に変換
```
E[y_1] = E_X[E[y_1 | D=1, X]]
```

**Step 4**: 重み付けの導入

ここで巧妙なトリックを使う:
```
E[y_1 | D=1, X] = E[y | D=1, X]  （D=1のとき y = y_1）
```

これを全体の期待値に変換するため:
```
E_X[E[y | D=1, X]] = E[E[y | D=1, X] | D=1] × P(D=1) + E[E[y | D=1, X] | D=0] × P(D=0)
```

しかし、D=0 のグループでは E[y | D=1, X] を直接計算できない。

**別のアプローチ**:
```
E[y_1] = E_X[E[y_1 | D=1, X]]
       = E_X[E[y | D=1, X]]
```

変形すると:
```
E[y_1] = E_X[E[D·y / P(X) | X]]  （Dで重み付け）
       = E[D·y / P(X)]
```

**導出の詳細**:
```
E[D·y / P(X) | X] = E[D | X] · E[y | D=1, X] / P(X)
                  = P(X) · E[y_1 | X] / P(X)
                  = E[y_1 | X]

E[D·y / P(X)] = E_X[E[D·y / P(X) | X]]
              = E_X[E[y_1 | X]]
              = E[y_1]
```

**E[y_0] の導出**:

同様に:
```
E[y_0] = E[(1-D)·y / (1-P(X))]
```

**ATE の IPW 推定量**:
```
ATE = E[y_1] - E[y_0]
    = E[D·y / P(X)] - E[(1-D)·y / (1-P(X))]
    = E[D·y / P(X) - (1-D)·y / (1-P(X))]
```

**なぜこの方法で推定できるのか**:

1. **重み付けの意味**:
   - D=1 かつ P(X) が低い人（処置を受けにくいのに受けた人）: 大きな重み 1/P(X)
   - D=1 かつ P(X) が高い人（処置を受けやすくて受けた人）: 小さな重み 1/P(X)
   - この重み付けにより、**疑似的なランダム化**を実現

2. **直感的説明**:
   - 処置を受けにくい特性なのに受けた人のデータは「貴重」なので、重みを大きくする
   - 処置を受けやすい特性で受けた人のデータは「ありふれている」ので、重みを小さくする
   - 結果として、処置群と非処置群のバランスを取る

**(3) 傾向スコアの推定手順**

**問題**: データには P(X) は含まれていないので、推定する必要がある

**Step 1**: モデルの選択

**プロビットモデル（Probit Model）**:
```
P(D=1 | X) = Φ(X'β)
```
- Φ は標準正規分布の累積分布関数

**ロジットモデル（Logit Model）**:
```
P(D=1 | X) = exp(X'β) / (1 + exp(X'β))
```

**Step 2**: 最尤法（Maximum Likelihood）で推定

D を X に回帰する（OLSではなく、プロビット/ロジット）:
```
D_i ~ Probit(X_i'β)  または  D_i ~ Logit(X_i'β)
```

推定結果: β̂

**Step 3**: 各個人の傾向スコアを計算
```
P̂(X_i) = Φ(X_i'β̂)  （プロビットの場合）
```

または

```
P̂(X_i) = exp(X_i'β̂) / (1 + exp(X_i'β̂))  （ロジットの場合）
```

**Step 4**: IPW推定量の計算
```
ÂTE = (1/n) Σ_{i=1}^n [D_i y_i / P̂(X_i) - (1-D_i) y_i / (1-P̂(X_i))]
```

**注意点**:
- P̂(X) が 0 や 1 に近い場合、1/P̂(X) や 1/(1-P̂(X)) が非常に大きくなる
- オーバーラップの仮定が重要（0 < P(X) < 1）
- P̂(X) が極端な値の場合、推定が不安定になる（trimming が必要な場合も）

**(4) 回帰による調整との比較**

**IPW法の利点**:

1. **y と X の関数形を仮定しなくてよい**:
   - 回帰調整: M_1(X), M_0(X) のモデルを正しく特定化する必要がある
   - IPW: y と X の関係を仮定しない
   - より柔軟（モデルフリー）

2. **非線形関係に頑健**:
   - y と X の関係が複雑でも、傾向スコアさえ正しく推定できればOK

**IPW法の限界**:

1. **傾向スコアのモデルに依存**:
   - P(X) のモデル（プロビット/ロジット）を正しく特定化する必要がある
   - D と X の関係を間違えると、P̂(X) が間違い、ATE の推定も間違う
   - 回帰調整と違う種類の特定化リスク

2. **オーバーラップが必要**:
   - 回帰調整: オーバーラップ不要
   - IPW: オーバーラップが破れると、1/P(X) が無限大に近づく
   - 極端な重みが推定を不安定にする

3. **分散が大きい**:
   - 重み 1/P(X) や 1/(1-P(X)) のばらつきが大きいと、推定量の分散が大きくなる
   - 特に、P(X) が 0 や 1 に近い場合

4. **解釈が難しい**:
   - 回帰調整: 予測値を比較するので直感的
   - IPW: 重み付け平均なので、やや解釈が難しい

**まとめ**:

| 項目 | 回帰による調整 | IPW法 |
|------|--------------|--------|
| 仮定するモデル | M_1(X), M_0(X) (y と X の関係) | P(X) (D と X の関係) |
| オーバーラップ | 不要 | 必要 |
| 利点 | 直感的、解釈しやすい | y と X の関数形を仮定不要 |
| 限界 | y と X のモデルに依存 | P(X) のモデルに依存、分散大 |

**どちらも「片方のモデル」を正しく特定化する必要がある**
→ この弱点を克服するのが、次の「二重にロバストな推定法」

---

### 問題7: 二重にロバストな推定法（Doubly Robust Estimation）

二重にロバストな推定法について説明する。

(1) この方法が「二重にロバスト（doubly robust）」と呼ばれる理由を説明しなさい。

(2) 回帰による調整（M_1, M_0）と IPW法（P(X)）を組み合わせた推定量の考え方を説明しなさい。

(3) この方法の利点と、なぜ実務で推奨されるのかを述べなさい。

#### 📖 ヒント
- 2つのモデル（M と P）のうち、どちらか一方が正しければATE を正しく推定できる
- 両方間違っていると失敗する
- 保険をかけた推定法

#### ✅ 解答

**(1) 「二重にロバスト」の意味**

**定義**:

二重にロバストな推定法は、以下の2つのモデルを使う:
1. **回帰モデル**: M_1(X) = E[y | D=1, X] と M_0(X) = E[y | D=0, X]
2. **傾向スコアモデル**: P(X) = P(D=1 | X)

**ロバスト性の意味**:

**「どちらか一方が正しければ、ATE を正しく推定できる」**

つまり:
- M_1, M_0 が正しく、P(X) が間違っていても → ✓ ATE は正しく推定できる
- M_1, M_0 が間違っていて、P(X) が正しくても → ✓ ATE は正しく推定できる
- **両方とも正しい** → ✓ もちろん ATE は正しく推定できる
- **両方とも間違っている** → ✗ ATE の推定は失敗する

**なぜ「二重に」ロバストか**:
- 2つのモデルのうち、1つだけ正しければ良い
- 2重の保険をかけている
- どちらのモデルに対しても頑健（robust）

**比較**:

| 方法 | 必要な条件 |
|------|----------|
| 回帰による調整 | M_1, M_0 が正しい |
| IPW法 | P(X) が正しい |
| **二重ロバスト** | **M_1, M_0 または P(X) のどちらか一方が正しい** |

**(2) 推定量の考え方**

**基本的なアイデア**:

回帰調整に、IPW による「補正項」を追加する。

**推定量の形（簡略版）**:
```
ÂTE = (1/n) Σ_i [M̂_1(X_i) - M̂_0(X_i) + W_i]
```

ここで W_i は補正項:
```
W_i = D_i(y_i - M̂_1(X_i))/P̂(X_i) - (1-D_i)(y_i - M̂_0(X_i))/(1-P̂(X_i))
```

**完全な形**:
```
ÂTE = (1/n) Σ_{i=1}^n [
    M̂_1(X_i) - M̂_0(X_i)
    + D_i(y_i - M̂_1(X_i))/P̂(X_i)
    - (1-D_i)(y_i - M̂_0(X_i))/(1-P̂(X_i))
]
```

**3つの項の意味**:

1. **第1項**: M̂_1(X_i) - M̂_0(X_i)
   - 回帰による調整の部分
   - 各個人の予測される処置効果

2. **第2項**: D_i(y_i - M̂_1(X_i))/P̂(X_i)
   - 処置群（D=1）の残差を傾向スコアで重み付け
   - 「実際の y」と「予測値 M̂_1」の差（予測誤差）を補正

3. **第3項**: -(1-D_i)(y_i - M̂_0(X_i))/(1-P̂(X_i))
   - 対照群（D=0）の残差を傾向スコアで重み付け
   - 「実際の y」と「予測値 M̂_0」の差（予測誤差）を補正

**なぜこれが二重にロバストか**:

**ケース1: M̂_1, M̂_0 が正しい場合**

もし M̂_1(X) = E[y | D=1, X] が真のモデルならば:
```
E[y_i - M̂_1(X_i) | D_i=1, X_i] = 0
```

つまり、予測誤差の期待値はゼロ。

したがって、第2項と第3項の期待値は 0 になり:
```
E[ÂTE] = E[M̂_1(X) - M̂_0(X)] = ATE
```

→ P̂(X) が間違っていても、補正項の期待値が 0 なので、影響なし

**ケース2: P̂(X) が正しい場合**

もし P̂(X) = P(D=1|X) が真のモデルならば、IPW の性質により:
```
E[D_i(y_i - M̂_1(X_i))/P̂(X_i) - (1-D_i)(y_i - M̂_0(X_i))/(1-P̂(X_i))]
```

が、M̂_1, M̂_0 の誤差を補正してくれる。

結果として:
```
E[ÂTE] = ATE
```

→ M̂_1, M̂_0 が間違っていても、補正項が修正してくれる

**ケース3: 両方間違っている場合**

残念ながら、この場合は一般に E[ÂTE] ≠ ATE となり、推定は失敗する。

**(3) 利点と実務での推奨理由**

**利点**:

1. **頑健性（Robustness）**:
   - どちらか一方のモデルが正しければ良い
   - モデルの誤定式化のリスクを軽減
   - 「保険をかけた」推定法

2. **柔軟性**:
   - M のモデルに自信がなくても、P を正しく推定できれば大丈夫
   - P のモデルに自信がなくても、M を正しく推定できれば大丈夫
   - 完璧なモデルがなくても使える

3. **効率性**:
   - 両方のモデルが正しい場合、最も効率的な推定量（分散が最小）
   - 情報を最大限に活用

4. **実用性**:
   - 実務では、どちらのモデルが正しいか分からないことが多い
   - 両方試してみて、どちらかが当たればOK
   - 成功する可能性が2倍

**実務で推奨される理由**:

1. **不確実性への対処**:
   - y と X の関係も、D と X の関係も、完璧には分からない
   - 複数のモデルを試して、ロバストネスをチェックする文化
   - 二重ロバストは、この文化に最も適した方法

2. **計算コストは大きくない**:
   - 回帰調整と IPW の両方を計算するだけ
   - 現代のコンピュータでは容易

3. **学術的に評価が高い**:
   - 多くの計量経済学者が推奨
   - 査読付き論文で使われることが多い
   - 因果推論のベストプラクティス

4. **失敗のリスクを減らす**:
   - 回帰調整だけ: M が間違うと失敗
   - IPW だけ: P が間違うと失敗
   - 二重ロバスト: 両方間違わない限り成功
   - 失敗の確率が減る

**実務での使い方**:

1. 両方のモデル（M と P）を慎重に構築
2. 様々な変数の組み合わせや関数形を試す
3. 二重ロバスト推定量を計算
4. 結果の安定性をチェック（ロバストネスチェック）
5. どちらか一方が正しいことを期待して報告

**限界**:

1. **万能ではない**:
   - 両方のモデルが間違っていれば失敗
   - 「どちらか一方は正しい」という保証はない

2. **解釈が複雑**:
   - 3つの項があり、直感的理解が難しい
   - 一般の人に説明しにくい

3. **CIAへの依存**:
   - どの方法を使っても、CIAが成立しなければバイアスは残る
   - 観測できない交絡変数があればアウト

**まとめ**:
二重ロバスト推定法は、回帰調整と IPW の「いいとこ取り」をした方法で、実務で最も推奨される因果推論の手法の1つ。

---

## Part 4: 操作変数法（IV法）

### 問題8: 内生性の発生原因

OLS（最小二乗法）が使えない状況、すなわち説明変数 X と誤差項 U が相関する「内生性」について説明する。

(1) 内生性が発生する3つの主な原因を挙げ、それぞれを具体例とともに説明しなさい。

(2) なぜ内生性があると、OLS推定量がバイアスを持つのか、数学的に説明しなさい。

(3) これらの問題を解決するために、操作変数法が必要な理由を述べなさい。

#### 📖 ヒント
- 欠落変数、同時方程式、測定誤差
- E[u | X] ≠ 0 が問題
- Cov(X, u) ≠ 0 → バイアス

#### ✅ 解答

**(1) 内生性の3つの原因**

**原因1: 欠落変数（Omitted Variable）**

**説明**:
- 本当はモデルに必要な変数 Z がデータになく、モデルから欠落している
- Z が X とも y とも相関している場合、Z の影響が誤差項 u に含まれる
- 結果として X と u が相関する

**具体例: 教育と賃金**

真のモデル:
```
賃金 = β_0 + β_1・教育年数 + β_2・能力 + u
```

しかし、「能力」のデータがないため、推定するモデル:
```
賃金 = β_0 + β_1・教育年数 + v
```

ここで v = β_2・能力 + u

問題点:
- 能力が高い人は、教育年数も多く、賃金も高い
- 「教育年数」と「能力」が正の相関
- 「能力」が誤差項 v に含まれる
- したがって、Cov(教育年数, v) ≠ 0
- β̂_1 は教育の真の効果だけでなく、能力の効果も拾ってしまう（過大推定）

---

**原因2: 同時方程式（Simultaneity / Reverse Causality）**

**説明**:
- y と X がお互いに影響しあって同時に決まる
- X → y だけでなく、y → X という逆の因果関係も存在
- 結果として X と誤差項 u が相関する

**具体例: 需要と供給**

需要関数を推定したい:
```
需要量 Q = α - βP + u_d  （価格 P が上がると需要 Q が減る）
```

しかし、市場では需要と供給が同時に決まる:
```
需要: Q = α - βP + u_d
供給: Q = γ + δP + u_s
均衡: 需要 = 供給
```

問題点:
- u_d が大きい（需要ショック、例：流行）→ Q が増える → P も上がる
- つまり、u_d → P という関係が発生
- 説明変数 P と誤差項 u_d が相関
- OLS で推定すると、需要曲線と供給曲線の識別ができない

**別の例: 警察と犯罪**

モデル:
```
犯罪率 = β_0 + β_1・警察の数 + u
```

問題:
- 本当は「警察が多い → 犯罪が減る」を知りたい（β_1 < 0 を期待）
- しかし現実は「犯罪が多い地域 → 警察を増やす」という政策
- 犯罪率 → 警察の数 という逆の因果関係
- 高犯罪地域（u が大）に警察が多い → Cov(警察の数, u) > 0
- OLS で推定すると β̂_1 > 0 となる可能性（警察が多いと犯罪が増える？）

---

**原因3: 測定誤差（Measurement Error）**

**説明**:
- 説明変数 X を正確に測定できず、誤差を含んだデータを観測
- 真の値 X* と観測値 X = X* + V（V は測定誤差）がズレている
- 測定誤差 V が誤差項に含まれると、X と誤差項が相関

**具体例: 所得と消費**

真のモデル:
```
消費 = β_0 + β_1・真の所得(X*) + u
```

しかし、所得は自己申告などで測定誤差がある:
```
観測される所得: X = X* + V  （V は測定誤差、例：申告ミス、記憶違い）
```

推定するモデル:
```
消費 = β_0 + β_1・X + (u - β_1 V)
      = β_0 + β_1・X + ε
```

ここで ε = u - β_1 V

問題点:
- 説明変数 X = X* + V と誤差項 ε = u - β_1 V は、共通の V を含む
- したがって、Cov(X, ε) = Cov(X* + V, u - β_1 V) = -β_1 Var(V) ≠ 0
- β̂_1 は真の値 β_1 よりも **ゼロ方向にバイアス**（減衰バイアス; attenuation bias）

**(2) 内生性があるとOLSがバイアスを持つ理由**

**OLSの性質**:

OLS推定量:
```
β̂_1 = Σ(X_i - X̄)(y_i - ȳ) / Σ(X_i - X̄)²
```

確率極限:
```
plim(β̂_1) = β_1 + Cov(X, u) / Var(X)
```

**導出**:

真のモデル: y = β_0 + β_1 X + u

OLS推定量の確率極限:
```
plim(β̂_1) = plim[(1/n)Σ X_i y_i - X̄·ȳ] / plim[(1/n)Σ X_i² - X̄²]
           = Cov(X, y) / Var(X)
```

y = β_0 + β_1 X + u を代入:
```
Cov(X, y) = Cov(X, β_0 + β_1 X + u)
          = β_1 Var(X) + Cov(X, u)
```

したがって:
```
plim(β̂_1) = [β_1 Var(X) + Cov(X, u)] / Var(X)
           = β_1 + Cov(X, u) / Var(X)
```

**バイアス**:
```
Bias = plim(β̂_1) - β_1 = Cov(X, u) / Var(X)
```

**結論**:
- Cov(X, u) = 0 ならば、Bias = 0（OLSは一致推定量）
- **Cov(X, u) ≠ 0 ならば、Bias ≠ 0**（OLSはバイアスを持つ）
- Cov(X, u) > 0 なら、過大推定
- Cov(X, u) < 0 なら、過小推定

**バイアスの性質**:
- このバイアスは、サンプルサイズ n を大きくしても消えない
- 一致性が失われる
- どれだけデータを集めても、推定値は真の値に収束しない

**(3) 操作変数法が必要な理由**

**OLSの限界**:
- 内生性がある場合、OLSは使えない
- E[u | X] = 0 という最も重要な仮定が破れている
- どれだけサンプルを増やしても、バイアスは残る

**操作変数法の役割**:

1. **内生変数の「良い部分」だけを抽出**:
   - X の変動のうち、u と相関していない「外生的な部分」だけを使う
   - これにより、Cov(X, u) ≠ 0 の問題を回避

2. **操作変数 Z の利用**:
   - X と相関するが、u とは相関しない変数 Z を見つける
   - Z を通じた X の変動だけを使って β_1 を推定

3. **因果関係の識別**:
   - 同時方程式など、X と y が相互に影響する場合でも
   - Z を使うことで、X → y の因果効果を識別できる

4. **測定誤差の問題も解決**:
   - X に測定誤差があっても、Z が真の X* と相関していれば
   - β_1 を一致推定できる

**まとめ**:

| 問題 | OLS | 操作変数法 |
|------|-----|----------|
| 欠落変数 | バイアス | Z が欠落変数と無相関なら一致推定 |
| 同時方程式 | 識別不可能 | Z を使って識別可能 |
| 測定誤差 | 減衰バイアス | Z が真の X* と相関すれば一致推定 |

操作変数法は、内生性の問題を解決するための**唯一の方法**（RCTができない場合）

---

### 問題9: 測定誤差の詳細分析

説明変数に測定誤差がある場合の内生性について、詳しく分析する。

真のモデル:
```
y = β_0 + β_1 X* + u
```

しかし、真の値 X* は観測できず、測定誤差を含む X を観測:
```
X = X* + V
```

ここで、u と V は互いに独立、X* とも独立とする。

(1) 観測される X を使った回帰式を、u と V で表しなさい。

(2) 新しい誤差項と説明変数 X の共分散 Cov(X, ε) を計算し、内生性が発生することを示しなさい。

(3) OLS推定量 β̂_1 の確率極限を求め、どのようなバイアスが発生するか説明しなさい（減衰バイアス）。

#### 📖 ヒント
- X* = X - V を真のモデルに代入
- Cov(X, ε) = Cov(X* + V, u - β_1 V) を計算
- plim(β̂_1) = β_1 + Cov(X, ε) / Var(X)

#### ✅ 解答

**(1) 観測される X を使った回帰式**

**Step 1**: 真のモデル
```
y = β_0 + β_1 X* + u
```

**Step 2**: X* を観測される X で表す

測定誤差の関係:
```
X = X* + V
```

したがって:
```
X* = X - V
```

**Step 3**: 真のモデルに代入
```
y = β_0 + β_1(X - V) + u
  = β_0 + β_1 X - β_1 V + u
  = β_0 + β_1 X + (u - β_1 V)
```

**Step 4**: 新しい誤差項を定義
```
ε = u - β_1 V
```

**観測される X を使った回帰式**:
```
y = β_0 + β_1 X + ε
```

ここで ε = u - β_1 V

**(2) 内生性の発生の証明**

**目標**: Cov(X, ε) を計算する

**Step 1**: X と ε を展開
```
X = X* + V
ε = u - β_1 V
```

**Step 2**: 共分散の計算
```
Cov(X, ε) = Cov(X* + V, u - β_1 V)
```

**Step 3**: 共分散の線形性を使う
```
Cov(X* + V, u - β_1 V) = Cov(X*, u - β_1 V) + Cov(V, u - β_1 V)
```

**Step 4**: 各項を計算

**第1項**: Cov(X*, u - β_1 V)
```
= Cov(X*, u) - β_1 Cov(X*, V)
```

仮定より、X* と u は独立、X* と V も独立なので:
```
= 0 - β_1 × 0 = 0
```

**第2項**: Cov(V, u - β_1 V)
```
= Cov(V, u) - β_1 Cov(V, V)
= Cov(V, u) - β_1 Var(V)
```

仮定より、V と u は独立なので、Cov(V, u) = 0:
```
= 0 - β_1 Var(V)
= -β_1 Var(V)
```

**Step 5**: 合計
```
Cov(X, ε) = 0 + (-β_1 Var(V))
          = -β_1 Var(V)
```

**結論**:
```
Cov(X, ε) = -β_1 Var(V) ≠ 0  （β_1 ≠ 0 かつ Var(V) > 0 ならば）
```

**内生性が発生**: 説明変数 X と誤差項 ε が相関している

**(3) OLS推定量のバイアス（減衰バイアス）**

**Step 1**: OLS推定量の確率極限

一般的な公式:
```
plim(β̂_1) = β_1 + Cov(X, ε) / Var(X)
```

**Step 2**: Cov(X, ε) を代入

(2) の結果より:
```
plim(β̂_1) = β_1 + (-β_1 Var(V)) / Var(X)
           = β_1 - β_1 Var(V) / Var(X)
           = β_1 [1 - Var(V) / Var(X)]
```

**Step 3**: Var(X) を展開

X = X* + V なので（X* と V は独立）:
```
Var(X) = Var(X* + V) = Var(X*) + Var(V)
```

**Step 4**: 代入
```
plim(β̂_1) = β_1 [1 - Var(V) / (Var(X*) + Var(V))]
           = β_1 [(Var(X*) + Var(V) - Var(V)) / (Var(X*) + Var(V))]
           = β_1 [Var(X*) / (Var(X*) + Var(V))]
```

**Step 5**: 減衰因子を定義

λ = Var(X*) / Var(X) = Var(X*) / (Var(X*) + Var(V))

すると:
```
plim(β̂_1) = β_1 · λ
```

ここで 0 < λ < 1（測定誤差がある場合）

**バイアス**:
```
Bias = plim(β̂_1) - β_1 = β_1 · λ - β_1 = β_1(λ - 1)
```

λ < 1 なので、Bias < 0（β_1 > 0 の場合）

**減衰バイアス（Attenuation Bias）の性質**:

1. **ゼロ方向へのバイアス**:
   - β_1 > 0 の場合、β̂_1 は過小推定（真の値より小さい）
   - β_1 < 0 の場合、β̂_1 は過大推定（絶対値が小さくなる）
   - いずれも「ゼロに近づく」バイアス

2. **減衰因子 λ の意味**:
   ```
   λ = Var(X*) / (Var(X*) + Var(V)) = シグナル / (シグナル + ノイズ)
   ```
   - 測定誤差 V が大きいほど、λ は小さくなる
   - λ が小さいほど、バイアスは大きい（β̂_1 は 0 に近づく）

3. **測定誤差の影響**:
   - Var(V) = 0（測定誤差なし）→ λ = 1 → バイアスなし
   - Var(V) が大きい → λ が小さい → バイアス大

**数値例**:

真の値: β_1 = 2
```
ケース1: Var(X*) = 9, Var(V) = 1
  → λ = 9/10 = 0.9
  → plim(β̂_1) = 2 × 0.9 = 1.8

ケース2: Var(X*) = 5, Var(V) = 5
  → λ = 5/10 = 0.5
  → plim(β̂_1) = 2 × 0.5 = 1.0

ケース3: Var(X*) = 1, Var(V) = 9
  → λ = 1/10 = 0.1
  → plim(β̂_1) = 2 × 0.1 = 0.2
```

測定誤差が大きいほど、推定値は真の値から大きくズレる。

**まとめ**:
- 測定誤差がある場合、OLS推定量は減衰バイアスを持つ
- 推定値は真の値よりもゼロに近くなる
- 操作変数法を使えば、この問題を解決できる

---

### 問題10: 操作変数（IV）の条件と推定

内生性の問題を解決するために、操作変数（Instrumental Variable, IV）を用いる。

真のモデル:
```
y = β_0 + β_1 X + u
```

X と u が相関している（内生性）ため、OLSは使えない。操作変数 Z を用いて β_1 を推定する。

(1) 操作変数 Z が満たすべき2つの条件を、数式と言葉で説明しなさい。

(2) 単純なIV推定量（X, Z が各1変数の場合）の導出過程を示しなさい。

(3) 2段階最小二乗法（2SLS / TSLS）の推定手順を詳しく説明し、なぜこの方法で β_1 を一致推定できるのか説明しなさい。

(4) 良い操作変数を見つけることの難しさと、実務での課題を述べなさい。

#### 📖 ヒント
- 関連性: Cov(Z, X) ≠ 0
- 外生性: Cov(Z, u) = 0
- β_1 = Cov(Z, y) / Cov(Z, X)
- 第1段階で X̂ を作り、第2段階で y を X̂ に回帰

#### ✅ 解答

**(1) 操作変数の2つの条件**

操作変数 Z が満たすべき条件:

**条件1: 関連性（Relevance）**

**数式**:
```
Cov(Z, X) ≠ 0
```

**言葉**:
- 操作変数 Z は、内生変数 X と**相関していなければならない**
- Z が X を説明する力を持つ
- Z の変動が X の変動を引き起こす

**なぜ必要か**:
- Z が X と無相関なら、Z は X に関する情報を持たない
- X を Z で説明できない
- β_1 の推定に役立たない
- 後述の IV 推定量の分母が 0 になってしまう

**第1段階の回帰で確認**:
```
X = π_0 + π_1 Z + v
```

この回帰で、π_1 ≠ 0 かつ統計的に有意であることが必要。

**弱い操作変数（Weak IV）の問題**:
- Cov(Z, X) が 0 ではないが、非常に小さい場合
- 推定量の分散が大きくなり、推定が不安定
- F統計量が 10 以上であることが目安（Stock & Yogo, 2005）

---

**条件2: 外生性（Exogeneity）**

**数式**:
```
Cov(Z, u) = 0
```

または

```
E[u | Z] = 0
```

**言葉**:
- 操作変数 Z は、誤差項 u と**相関してはならない**
- Z は y に対して、X を通じてのみ影響する
- Z が y に直接影響する経路があってはならない（**除外制約; exclusion restriction**）

**なぜ必要か**:
- Z が u と相関していると、Z 自体が内生変数になってしまう
- OLS と同じ問題が発生
- β_1 の推定にバイアスが生じる

**因果ダイアグラム**:
```
Z → X → y
       ↑
       u

OK: Z は X を通じてのみ y に影響
```

```
Z → X → y
 ↘     ↑
   ↘   u
    ↘↗

NG: Z が u（または y）に直接影響している
```

**問題点**:
- この条件は**検証不可能**（u は観測できないため）
- 経済理論や常識に基づいて**仮定**するしかない
- IV 推定の最大の弱点

---

**まとめ**:

| 条件 | 数式 | 意味 | 検証可能性 |
|------|------|------|-----------|
| **関連性** | Cov(Z, X) ≠ 0 | Z は X と相関 | ✓ データで検証可能 |
| **外生性** | Cov(Z, u) = 0 | Z は u と無相関 | ✗ 検証不可能（仮定） |

**(2) IV推定量の導出**

**元のモデル**:
```
y = β_0 + β_1 X + u
```

問題: Cov(X, u) ≠ 0（内生性）

**Step 1**: モデルの両辺と Z の共分散をとる
```
Cov(Z, y) = Cov(Z, β_0 + β_1 X + u)
```

**Step 2**: 共分散の性質を使う

共分散の線形性:
```
Cov(Z, y) = Cov(Z, β_0) + Cov(Z, β_1 X) + Cov(Z, u)
```

定数との共分散は 0:
```
Cov(Z, β_0) = 0
```

係数を外に出す:
```
Cov(Z, β_1 X) = β_1 Cov(Z, X)
```

したがって:
```
Cov(Z, y) = β_1 Cov(Z, X) + Cov(Z, u)
```

**Step 3**: IV の条件を使う

外生性の条件: Cov(Z, u) = 0

したがって:
```
Cov(Z, y) = β_1 Cov(Z, X)
```

**Step 4**: β_1 について解く

関連性の条件より Cov(Z, X) ≠ 0 なので、両辺を Cov(Z, X) で割れる:
```
β_1 = Cov(Z, y) / Cov(Z, X)
```

**Step 5**: IV推定量

データ（標本）からは、標本共分散 S_{ZY} と S_{ZX} を計算:
```
S_{ZY} = (1/n) Σ (Z_i - Z̄)(y_i - ȳ)
S_{ZX} = (1/n) Σ (Z_i - Z̄)(X_i - X̄)
```

**IV推定量**:
```
β̂_{1,IV} = S_{ZY} / S_{ZX}
```

または

```
β̂_{1,IV} = [Σ(Z_i - Z̄)(y_i - ȳ)] / [Σ(Z_i - Z̄)(X_i - X̄)]
```

**一致性の確認**:
```
plim(β̂_{1,IV}) = plim(S_{ZY}) / plim(S_{ZX})
                = Cov(Z, y) / Cov(Z, X)
                = [β_1 Cov(Z, X) + Cov(Z, u)] / Cov(Z, X)
                = β_1 + Cov(Z, u) / Cov(Z, X)
```

IV の外生性条件 Cov(Z, u) = 0 より:
```
plim(β̂_{1,IV}) = β_1
```

**結論**: IV推定量は β_1 の一致推定量である

**(3) 2段階最小二乗法（2SLS / TSLS）の推定手順**

2SLSは、IV推定量を計算する、より一般的な（そして直感的な）方法。

**基本的な考え方**:

内生変数 X の変動は、2つの部分に分解できる:
1. **外生的な部分**: Z と相関する部分（「良い」変動）
2. **内生的な部分**: u と相関する部分（「悪い」変動）

2SLS は、**「良い」部分だけを抽出して回帰に使う**

---

**第1段階（1st Stage）: X の外生的部分を抽出**

**Step 1**: 内生変数 X を、操作変数 Z に OLS 回帰する
```
X = π_0 + π_1 Z + v
```

**Step 2**: OLS で π̂_0 と π̂_1 を推定

**Step 3**: X の予測値（fitted value）を計算
```
X̂_i = π̂_0 + π̂_1 Z_i
```

**X̂ の意味**:
- X̂ は、X の変動のうち、Z で説明できる部分
- つまり、X の「外生的な」部分、「良い」変動
- X̂ は Z と完全に相関し（Z の線形関数なので）
- X̂ は u とは無相関（Z が u と無相関だから）

**第1段階の確認**:
- π̂_1 が統計的に有意か確認（F 検定）
- F 統計量が 10 以上であることが望ましい
- そうでない場合、「弱い操作変数」の問題

---

**第2段階（2nd Stage）: 予測値を使って回帰**

**Step 1**: 元の y を、元の X の代わりに**予測値 X̂** に OLS 回帰する
```
y = β_0 + β_1 X̂ + ε
```

**注意**: ここで使うのは X ではなく X̂

**Step 2**: OLS で β̂_0 と β̂_1 を推定

この β̂_1 が **2SLS 推定量**

---

**なぜこの方法で β_1 を一致推定できるのか**:

**Step 1**: 第2段階のモデルに真のモデルを代入

真のモデル: y = β_0 + β_1 X + u

これを第2段階の式と比較:
```
y = β_0 + β_1 X̂ + ε
```

ε を定義すると:
```
ε = y - β_0 - β_1 X̂
  = (β_0 + β_1 X + u) - β_0 - β_1 X̂
  = β_1(X - X̂) + u
```

**Step 2**: X̂ と ε の無相関性を確認

X̂ = π̂_0 + π̂_1 Z なので、X̂ は Z の線形関数。

ε = β_1(X - X̂) + u

```
Cov(X̂, ε) = Cov(X̂, β_1(X - X̂) + u)
           = β_1 Cov(X̂, X - X̂) + Cov(X̂, u)
```

第1項: X̂ は X の予測値なので、予測値と残差は無相関（OLSの性質）
```
Cov(X̂, X - X̂) = 0
```

第2項: X̂ は Z の関数で、Z は u と無相関なので
```
Cov(X̂, u) = 0
```

したがって:
```
Cov(X̂, ε) = 0
```

**Step 3**: OLSの性質を使う

第2段階の回帰:
```
y = β_0 + β_1 X̂ + ε
```

Cov(X̂, ε) = 0 なので、E[ε | X̂] = 0 という OLS の仮定が成立。

したがって、β̂_1 は β_1 の一致推定量:
```
plim(β̂_{1,2SLS}) = β_1
```

**結論**: 2SLS推定量は、IV推定量と同じく、β_1 を一致推定する

---

**2SLSとIV推定量の関係**:

X と Z が各1変数の場合、2SLS推定量と単純なIV推定量は**完全に一致する**:
```
β̂_{1,2SLS} = β̂_{1,IV} = S_{ZY} / S_{ZX}
```

X や Z が複数変数の場合、2SLS の方が一般的な方法として使われる。

**(4) 良い操作変数を見つける難しさと実務での課題**

**難しさ**:

1. **外生性の検証不可能性**:
   - Cov(Z, u) = 0 は検証できない（u は観測できない）
   - 経済理論や常識に基づいて「信じる」しかない
   - 査読者や読者を説得する必要がある
   - 批判されやすい

2. **除外制約の妥当性**:
   - Z は y に対して X を通じてのみ影響する、という仮定
   - しかし現実には、Z が y に直接影響する経路がある可能性
   - この経路を全て排除できているか、常に疑問が残る

**具体例: 教育の収益率**

モデル:
```
賃金 = β_0 + β_1・教育年数 + u
```

問題: 能力が高い人が教育を多く受ける → 内生性

**操作変数の候補1: 親の学歴**
- 関連性: ✓ 親の学歴が高いと子も教育を多く受ける
- 外生性: ✗？ 親の学歴は子の能力（遺伝や家庭環境）にも影響する可能性
- 除外制約が怪しい

**操作変数の候補2: 居住地の大学までの距離**
- 関連性: ✓ 大学が近いと進学しやすい
- 外生性: ✗？ 都市部（大学が近い）と田舎では労働市場が違う可能性
- 除外制約が怪しい

**操作変数の候補3: 義務教育年数の変更**
- 関連性: ✓ 法律で義務教育が延長されれば教育年数が増える
- 外生性: ✓ 法改正は個人の能力とは無関係（より妥当）
- 除外制約が比較的納得できる

→ しかし、このようなIVは滅多に見つからない

3. **関連性の弱さ（Weak IV）**:
   - Z が X と弱くしか相関していない場合
   - 推定量の分散が大きくなり、推定が不安定
   - 標準誤差が膨大になり、統計的に有意にならない
   - バイアスも発生する可能性

4. **創造性が必要**:
   - 良いIVを見つけるには、経済理論、制度、歴史の深い理解が必要
   - データを眺めるだけでは見つからない
   - 研究者の創意工夫が問われる

**実務での課題**:

1. **データの制約**:
   - 理想的なIVを思いついても、データがない場合が多い
   - 自然実験（natural experiment）を探すのが難しい

2. **複数のIV候補**:
   - 複数のIV候補がある場合、どれを使うべきか
   - 過剰識別検定（overidentification test）でチェック
   - しかし、全て外生性を満たさない可能性も

3. **説得力**:
   - 学会や査読で、IVの妥当性を巡って議論になることが多い
   - 「本当に外生性が成立しているのか？」という批判
   - 頑健性チェック（様々なIVで結果が安定しているか）が重要

4. **局所的な効果（LATE; Local Average Treatment Effect）**:
   - IV推定量は、「IVによって処置が変化した人たち」の平均効果を推定
   - ATE（全員の平均効果）とは異なる可能性
   - 外的妥当性（external validity）の問題

5. **トレードオフ**:
   - OLS: 内生性があるが、効率的（分散小）
   - IV: 内生性を解決するが、非効率的（分散大）
   - IVが弱いと、OLSよりも悪い結果になることも

**まとめ**:
- 良いIVを見つけることは、因果推論における最大の挑戦の1つ
- 外生性は検証できないため、理論的な説得力が鍵
- 実務では、自然実験や制度変更を利用したIVが評価される
- IVは「銀の弾丸」ではなく、慎重に使うべき道具

---

## 🎯 学習到達度チェックリスト

### レベル1: 基礎概念の理解
- [ ] 潜在的結果 y_0, y_1 の意味を説明できる
- [ ] 因果推論の根本的問題を理解している
- [ ] ATE, ATT, ATU の違いを説明できる
- [ ] RCT の基本的な仮定を説明できる
- [ ] CIA（条件付き独立性の仮定）の意味を理解している
- [ ] オーバーラップの仮定の意味を説明できる
- [ ] 内生性の3つの原因（欠落変数、同時方程式、測定誤差）を説明できる
- [ ] 操作変数の2つの条件（関連性、外生性）を説明できる

### レベル2: 数学的理解と計算
- [ ] RCT のもとで ATE = E[y | D=1] - E[y | D=0] を証明できる
- [ ] CIA のもとで条件付き ATE が推定できることを示せる
- [ ] 回帰による調整の手順を説明できる
- [ ] 傾向スコアの定義と IPW 推定量の導出を理解している
- [ ] 二重ロバスト推定法の仕組みを説明できる
- [ ] 測定誤差による減衰バイアスを導出できる
- [ ] IV 推定量の導出を示せる
- [ ] 2SLS の手順を説明し、なぜ一致推定量になるか説明できる

### レベル3: 批判的思考と応用
- [ ] 観察データでの因果推論の限界を理解している
- [ ] CIA が成立しない状況を具体的に議論できる
- [ ] 回帰調整、IPW、二重ロバストの利点と限界を比較できる
- [ ] 外的妥当性と一般均衡効果の問題を説明できる
- [ ] 良い操作変数を見つける難しさを理解している
- [ ] 弱い操作変数の問題を説明できる
- [ ] 実際の研究例を挙げて、IV の妥当性を議論できる
- [ ] 因果推論の各手法をどのような状況で使うべきか判断できる

---

## 💡 重要ポイントまとめ

### 🔥 試験で最も重要な10点

1. **潜在的結果モデル**
   - y_0 と y_1 の片方しか観測できない（根本的問題）
   - τ = y_1 - y_0（個人の因果効果）
   - ATE = E[y_1 - y_0]（集団の平均効果）

2. **RCTの威力**
   - D ⊥ (y_0, y_1)（完全な独立性）
   - ATE = E[y | D=1] - E[y | D=0]（単純比較でOK）
   - E[u | D] = 0 が成立 → OLS で因果効果を推定可能

3. **CIA（条件付き独立）**
   - (y_0, y_1) ⊥ D | X
   - X を条件付ければ、疑似的な RCT
   - **検証不可能な強い仮定**

4. **オーバーラップ**
   - 0 < P(D=1|X) < 1
   - どの X でも両グループが存在する
   - これがないと比較不可能

5. **回帰による調整**
   - M_1(X), M_0(X) を別々に推定
   - 異質な処置効果を捉えられる
   - y と X の関数形に依存（リスク）

6. **傾向スコア（IPW）**
   - P(X) = P(D=1|X)
   - 高次元の X を1次元に要約
   - y と X の関数形を仮定不要
   - オーバーラップが必要

7. **二重ロバスト推定法**
   - M と P のどちらか一方が正しければOK
   - 2重の保険
   - 実務で最も推奨される方法

8. **内生性の原因**
   - 欠落変数: Z が X とも y とも相関
   - 同時方程式: X ⇄ y の双方向因果
   - 測定誤差: X = X* + V → 減衰バイアス

9. **操作変数の条件**
   - 関連性: Cov(Z, X) ≠ 0（データで検証可能）
   - 外生性: Cov(Z, u) = 0（**検証不可能**）
   - 除外制約: Z → X → y（直接効果なし）

10. **2SLS（2段階最小二乗法）**
   - 第1段階: X を Z に回帰 → X̂ を作る
   - 第2段階: y を X̂ に回帰 → β̂_1
   - X の「良い」変動だけを使う

### 📌 よくある誤解

| 誤解 | 正しい理解 |
|------|-----------|
| 単純比較で因果効果が分かる | 選択バイアスがあるため不可（RCT以外） |
| ATE = ATT である | 一般には異なる（RCTなら等しい） |
| CIAは検証可能 | 検証不可能（信じるしかない） |
| オーバーラップは不要 | IPWでは必須（回帰調整では不要） |
| 二重ロバストは万能 | 両方のモデルが間違っていれば失敗 |
| 測定誤差はバイアスなし | 減衰バイアス（ゼロ方向）が発生 |
| IVの外生性は検証可能 | 検証不可能（IVの最大の弱点） |
| 弱いIVでも問題ない | 推定が不安定、バイアスも発生 |
| 2SLSとIVは違う方法 | X, Z が1変数なら完全に一致 |

---

## 📚 中間試験に向けて

### 試験範囲
- 因果推論の基礎理論（潜在的結果、ATE/ATT）
- RCT と観察データでの推定方法
- 回帰調整、傾向スコア、二重ロバスト
- 内生性の原因と対処法
- 操作変数法（IV推定量、2SLS）

### 重点的に復習すべき点
1. 数学的導出（証明問題）:
   - RCTでのATE = E[y|D=1] - E[y|D=0] の証明
   - IV推定量の導出
   - 測定誤差による減衰バイアスの導出

2. 概念的理解（論述問題）:
   - CIAとオーバーラップの違いと必要性
   - 各推定方法の利点と限界
   - 操作変数の2条件の意味

3. 計算問題:
   - 2SLSの手順（第1段階、第2段階）
   - 傾向スコアの推定とIPW推定量の計算

### 持ち込み
- **一切なし**（公式や定義を覚える必要あり）

---

**この練習問題を完璧にマスターすれば、中間試験は万全です！頑張ってください！** 🚀
